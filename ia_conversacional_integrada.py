#!/usr/bin/env python3
"""
IA Conversacional Integrada BMC Uruguay
Sistema de IA que aprende y evoluciona constantemente
"""

import datetime
import json
import os
import random
import re
from dataclasses import dataclass
from decimal import Decimal
from typing import Any, Optional, Dict, List

# Load environment variables from .env file
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass  # dotenv not available, will use system environment variables
from base_conocimiento_dinamica import BaseConocimientoDinamica, InteraccionCliente
from motor_analisis_conversiones import MotorAnalisisConversiones
from sistema_cotizaciones import (
    Cliente,
    EspecificacionCotizacion,
    SistemaCotizacionesBMC,
)
from utils_cotizaciones import (
    construir_contexto_validacion,
    formatear_mensaje_faltantes,
    obtener_datos_faltantes,
)
from tools_crm import CRMTools

# Unified Model Integrator
try:
    from model_integrator import get_model_integrator
    MODEL_INTEGRATOR_AVAILABLE = True
except ImportError:
    MODEL_INTEGRATOR_AVAILABLE = False
    print("Warning: Model integrator not available. Using pattern matching only.")

# Agent Workflows
try:
    from agent_workflows import quotation_agent, AgentContext
    AGENTS_AVAILABLE = True
except ImportError:
    AGENTS_AVAILABLE = False
    print("Warning: Agent workflows not available.")

# Knowledge Manager and Training System
try:
    from AI_AGENTS.EXECUTOR.knowledge_manager import KnowledgeManager
    from AI_AGENTS.EXECUTOR.training_system import TrainingSystem
    KNOWLEDGE_SYSTEM_AVAILABLE = True
except ImportError as e:
    KNOWLEDGE_SYSTEM_AVAILABLE = False
    print(f"Warning: Knowledge system not available: {e}")

# OpenAI integration (fallback)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


@dataclass
class ContextoConversacion:
    """Contexto de una conversaci√≥n en curso"""

    cliente_id: str
    sesion_id: str
    mensajes_intercambiados: list[dict[str, Any]]
    intencion_actual: str
    entidades_extraidas: dict[str, Any]
    estado_cotizacion: str
    datos_cliente: dict[str, Any]
    datos_producto: dict[str, Any]
    historial_interacciones: list[str]
    confianza_respuesta: float
    timestamp_inicio: datetime.datetime
    timestamp_ultima_actividad: datetime.datetime


@dataclass
class RespuestaIA:
    """Respuesta generada por la IA"""

    mensaje: str
    tipo_respuesta: str  # informativa, pregunta, cotizacion, seguimiento
    acciones_sugeridas: list[str]
    confianza: float
    fuentes_conocimiento: list[str]
    personalizacion: dict[str, Any]
    timestamp: datetime.datetime


class IAConversacionalIntegrada:
    """IA Conversacional que aprende y evoluciona constantemente"""

    # Memory limits to prevent unbounded growth
    MAX_CONVERSATIONS = 100  # Maximum number of active conversations to keep in memory
    MAX_MESSAGES_PER_CONVERSATION = 50  # Maximum messages per conversation
    CONVERSATION_TIMEOUT_HOURS = 24  # Remove conversations inactive for this many hours

    def __init__(self):
        self.base_conocimiento = BaseConocimientoDinamica()
        self.motor_analisis = MotorAnalisisConversiones(self.base_conocimiento)
        self.sistema_cotizaciones = SistemaCotizacionesBMC()
        self.crm_tools = CRMTools()
        self.conversaciones_activas = {}
        self.patrones_respuesta = {}
        self.entidades_reconocidas = {}
        
        # Initialize Knowledge Manager and Training System
        self.knowledge_manager = None
        self.training_system = None
        if KNOWLEDGE_SYSTEM_AVAILABLE:
            try:
                from pathlib import Path
                project_root = Path(__file__).parent
                self.knowledge_manager = KnowledgeManager(project_root=project_root)
                self.training_system = TrainingSystem(self.knowledge_manager)
                print("‚úÖ Knowledge Manager and Training System initialized")
            except Exception as e:
                print(f"‚ö†Ô∏è Error initializing knowledge system: {e}")

        # Shared context service for multi-agent system
        try:
            import sys
            from pathlib import Path

            python_scripts_path = Path(__file__).parent / "python-scripts"
            if str(python_scripts_path) not in sys.path:
                sys.path.insert(0, str(python_scripts_path))
            from shared_context_service import get_shared_context_service

            self.shared_context_service = get_shared_context_service()
            self.use_shared_context = True
        except Exception as e:
            self.shared_context_service = None
            self.use_shared_context = False
            print(f"Warning: Shared context service not available: {e}")

        # Unified Model Integrator configuration
        self.use_ai = False
        self.model_integrator = None
        self.openai_client = None  # Fallback
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

        # Try to initialize unified model integrator first
        if MODEL_INTEGRATOR_AVAILABLE:
            try:
                self.model_integrator = get_model_integrator()
                # Check if any models are available
                available_models = self.model_integrator.list_available_models()
                if available_models:
                    self.use_ai = True
                    print(f"‚úÖ Model integrator enabled with {len(available_models)} models")
                    for model in available_models:
                        if model['enabled']:
                            print(f"   - {model['provider']}: {model['model_name']}")
                else:
                    print("‚ö†Ô∏è No models configured in model integrator")
            except Exception as e:
                print(f"‚ö†Ô∏è Error initializing model integrator: {e}")
                self.use_ai = False
        
        # Fallback to OpenAI if integrator not available
        if not self.use_ai and OPENAI_AVAILABLE:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                try:
                    self.openai_client = OpenAI(api_key=api_key)
                    self.use_ai = True
                    print("‚úÖ OpenAI integration enabled (fallback mode)")
                except Exception as e:
                    print(f"‚ö†Ô∏è Error initializing OpenAI: {e}")
                    self.use_ai = False
            else:
                print("‚ö†Ô∏è No API keys configured, using pattern matching only")
        elif not self.use_ai:
            print("‚ö†Ô∏è No AI models available, using pattern matching only")

        self.cargar_configuracion_inicial()
    
    def _enriquecer_contexto_completo(self, mensaje: str, tema: Optional[str] = None) -> Dict:
        """Enriquece el contexto con base de conocimiento, documentaci√≥n y conversaciones similares"""
        contexto = {
            'productos': {},
            'documentacion': [],
            'conversaciones_similares': [],
            'patrones_venta': []
        }
        
        if not self.knowledge_manager:
            return contexto
        
        # 1. Cargar informaci√≥n de productos relevantes
        if tema:
            resultados = self.knowledge_manager.buscar_informacion_relevante(tema, max_results=5)
            contexto['productos'] = resultados.get('productos', [])
            contexto['documentacion'] = resultados.get('documentacion', [])
            contexto['conversaciones_similares'] = resultados.get('conversaciones', [])
        
        # 2. Obtener ejemplos few-shot
        if self.training_system:
            contexto['conversaciones_similares'] = self.training_system.encontrar_conversaciones_similares(
                mensaje, n=5
            )
        
        # 3. Obtener patrones de venta
        contexto['patrones_venta'] = self.knowledge_manager.obtener_patrones_venta()
        
        return contexto
    
    def _construir_system_prompt(self, contexto_enriquecido: Dict) -> str:
        """Construye el system prompt con toda la informaci√≥n disponible y perfil de experto"""
        prompt = """Eres 'Superchapita', un Consultor Senior en Soluciones Constructivas de BMC Uruguay.
Tu objetivo NO es seguir un guion, sino ENTENDER al cliente y ayudarlo a concretar su proyecto usando herramientas avanzadas.

**Tus Principios:**
1. **Cero Robots**: Nunca pidas datos como un formulario ("Ingrese largo x ancho"). Conversa naturalmente.
2. **Uso de Herramientas**: Si el cliente te da datos (ej. "necesito techo de 50m2"), USA tus herramientas para guardar el lead o cotizar. No preguntes lo que ya sabes.
3. **Proactividad**: Si falta informaci√≥n (ej. espesor), expl√≠cales POR QU√â la necesitas ("Para esa zona te recomiendo 100mm, ¬øte parece bien?").
4. **CRM como Verdad**: Todo dato √∫til (nombre, tel, proyecto) debe ser guardado inmediatamente en el CRM usando la herramienta 'save_lead_info'.

**Base de Conocimiento Actual:**
"""
        
        # Agregar base de conocimiento de productos
        if contexto_enriquecido.get('productos'):
            prompt += "INFORMACI√ìN DE PRODUCTOS:\n"
            for producto in contexto_enriquecido['productos'][:3]:
                prompt += f"- {json.dumps(producto, default=str, ensure_ascii=False)}\n"
            prompt += "\n"
        
        # Agregar patrones de venta
        if contexto_enriquecido.get('patrones_venta'):
            prompt += "TIPS DE VENTA:\n"
            for patron in contexto_enriquecido['patrones_venta'][:2]:
                prompt += f"- {patron}\n"
            prompt += "\n"
        
        prompt += """
**Instrucciones de Respuesta:**
- Habla como un experto uruguayo, profesional pero cercano.
- Usa emoji de vez en cuando (üë∑, üè†).
- Si vas a cotizar, confirma primero las medidas aproximadas.
- Si el cliente duda, ofrece asesoramiento t√©cnico."""
        
        return prompt

    def cargar_configuracion_inicial(self):
        """Carga la configuraci√≥n inicial de la IA"""
        # Configurar sistema de cotizaciones
        self.sistema_cotizaciones.actualizar_precio_producto("isodec", Decimal("150.00"))
        self.sistema_cotizaciones.actualizar_precio_producto("poliestireno", Decimal("120.00"))
        self.sistema_cotizaciones.actualizar_precio_producto("lana_roca", Decimal("140.00"))

        # Cargar patrones de respuesta iniciales
        self.patrones_respuesta = {
            "saludo": [
                "¬°Hola! Soy tu asistente de cotizaciones de BMC Uruguay. ¬øEn qu√© puedo ayudarte?",
                "¬°Buenos d√≠as! Estoy aqu√≠ para ayudarte con tus consultas de aislamiento t√©rmico.",
                "¬°Hola! ¬øTe interesa cotizar alg√∫n producto de aislamiento t√©rmico?",
            ],
            "despedida": [
                "¬°Gracias por contactar BMC Uruguay! Que tengas un excelente d√≠a.",
                "Ha sido un placer ayudarte. ¬°Hasta la pr√≥xima!",
                "Espero haber sido de ayuda. ¬°Que tengas un gran d√≠a!",
            ],
            "consulta_producto": [
                "Te ayudo con informaci√≥n sobre nuestros productos de aislamiento t√©rmico.",
                "Tenemos varios productos disponibles. ¬øCu√°l te interesa conocer?",
                "Perfecto, te explico las caracter√≠sticas de nuestros productos.",
            ],
            "cotizacion": [
                "¬°Excelente! Vamos a crear tu cotizaci√≥n paso a paso.",
                "Perfecto, necesito algunos datos para darte el precio exacto.",
                "Genial, te ayudo a cotizar el producto que necesitas.",
            ],
        }

        # Cargar entidades reconocidas
        self.entidades_reconocidas = {
            "productos": ["isodec", "poliestireno", "lana de roca", "lana_roca"],
            "espesores": ["50mm", "75mm", "100mm", "125mm", "150mm"],
            "colores": ["blanco", "gris", "personalizado"],
            "aplicaciones": [
                "casa",
                "edificio",
                "comercial",
                "industrial",
                "residencial",
            ],
            "objeciones": ["caro", "costoso", "no estoy seguro", "necesito pensarlo"],
            "intenciones": [
                "cotizar",
                "precio",
                "informacion",
                "caracteristicas",
                "instalacion",
            ],
        }

    def procesar_mensaje(self, mensaje: str, cliente_id: str, sesion_id: str = None) -> RespuestaIA:
        """Procesa un mensaje del cliente y genera respuesta"""
        if not sesion_id:
            sesion_id = f"sesion_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}"

        # Obtener o crear contexto de conversaci√≥n
        contexto = self._obtener_contexto_conversacion(cliente_id, sesion_id)

        # Actualizar contexto con nuevo mensaje
        self._actualizar_contexto(contexto, mensaje)

        # Updated to use Agentic Processing (LLM-first)
        # Bypassing legacy heuristic intent analysis for flow control,
        # relying on the Expert Agent to handle the conversation.
        respuesta = self._agentic_processing(mensaje, contexto)
        
        # We keep 'intencion' variable for backward compatibility with analytics logging below
        intencion = "agentic"
        entidades = {}

        # Registrar interacci√≥n
        self._registrar_interaccion(mensaje, respuesta, contexto)

        # Actualizar conocimiento
        self._actualizar_conocimiento_conversacion(contexto, respuesta)

        # Save to shared context service
        if self.use_shared_context and self.shared_context_service and contexto.sesion_id:
            try:
                # Add assistant message
                self.shared_context_service.add_message(
                    contexto.sesion_id,
                    respuesta.mensaje,
                    "assistant",
                    {
                        "intent": intencion,
                        "entities": entidades,
                        "confidence": respuesta.confianza,
                    },
                )
                # Save full context
                context_dict = {
                    "user_phone": contexto.cliente_id,
                    "cliente_id": contexto.cliente_id,
                    "intent": contexto.intencion_actual,
                    "entities": contexto.entidades_extraidas,
                    "quote_state": {
                        "estado": contexto.estado_cotizacion,
                        "datos_cliente": contexto.datos_cliente,
                        "datos_producto": contexto.datos_producto,
                    },
                    "messages": [
                        {
                            "role": msg.get("tipo") == "cliente" and "user" or "assistant",
                            "content": msg.get("mensaje", ""),
                            "timestamp": msg.get("timestamp", datetime.datetime.now()),
                        }
                        for msg in contexto.mensajes_intercambiados
                    ],
                }
                self.shared_context_service.save_context(contexto.sesion_id, context_dict)
            except Exception as e:
                print(f"Warning: Failed to save context to shared service: {e}")

        return respuesta

    def _obtener_contexto_conversacion(
        self, cliente_id: str, sesion_id: str
    ) -> ContextoConversacion:
        """Obtiene o crea el contexto de una conversaci√≥n"""
        clave_contexto = f"{cliente_id}_{sesion_id}"

        # Try to load from shared context service first
        if self.use_shared_context and self.shared_context_service and sesion_id:
            try:
                shared_context = self.shared_context_service.get_context(sesion_id, cliente_id)
                if shared_context:
                    # Convert shared context to ContextoConversacion
                    contexto = ContextoConversacion(
                        cliente_id=cliente_id,
                        sesion_id=sesion_id,
                        mensajes_intercambiados=[
                            {
                                "tipo": msg.get("role") == "user" and "cliente" or "asistente",
                                "mensaje": msg.get("content", ""),
                                "timestamp": msg.get("timestamp", datetime.datetime.now()),
                            }
                            for msg in shared_context.get("messages", [])
                        ],
                        intencion_actual=shared_context.get("intent", "general"),
                        entidades_extraidas=shared_context.get("entities", {}),
                        estado_cotizacion=shared_context.get("quote_state", {}).get(
                            "estado", "inicial"
                        ),
                        datos_cliente=shared_context.get("quote_state", {}).get(
                            "datos_cliente", {}
                        ),
                        datos_producto=shared_context.get("quote_state", {}).get(
                            "datos_producto", {}
                        ),
                        historial_interacciones=[],
                        confianza_respuesta=0.8,
                        timestamp_inicio=datetime.datetime.now(),
                        timestamp_ultima_actividad=datetime.datetime.now(),
                    )
                    # Store in active conversations for backward compatibility
                    self.conversaciones_activas[clave_contexto] = contexto
                    return contexto
            except Exception as e:
                print(f"Warning: Failed to load context from shared service: {e}")

        # Fallback to in-memory
        if clave_contexto in self.conversaciones_activas:
            return self.conversaciones_activas[clave_contexto]

        # Crear nuevo contexto
        contexto = ContextoConversacion(
            cliente_id=cliente_id,
            sesion_id=sesion_id,
            mensajes_intercambiados=[],
            intencion_actual="",
            entidades_extraidas={},
            estado_cotizacion="inicial",
            datos_cliente={},
            datos_producto={},
            historial_interacciones=[],
            confianza_respuesta=0.0,
            timestamp_inicio=datetime.datetime.now(),
            timestamp_ultima_actividad=datetime.datetime.now(),
        )

        self.conversaciones_activas[clave_contexto] = contexto
        
        # Periodically cleanup old conversations to prevent memory growth
        if len(self.conversaciones_activas) > self.MAX_CONVERSATIONS:
            self._limpiar_conversaciones_antiguas()
        
        return contexto

    def _limpiar_conversaciones_antiguas(self):
        """Remove old or inactive conversations to free memory"""
        if not self.conversaciones_activas:
            return
        
        now = datetime.datetime.now()
        timeout_delta = datetime.timedelta(hours=self.CONVERSATION_TIMEOUT_HOURS)
        
        # Remove conversations that are inactive for too long
        keys_to_remove = []
        for clave, contexto in self.conversaciones_activas.items():
            time_since_activity = now - contexto.timestamp_ultima_actividad
            if time_since_activity > timeout_delta:
                keys_to_remove.append(clave)
        
        # Remove old conversations
        for clave in keys_to_remove:
            del self.conversaciones_activas[clave]
        
        # If still over limit, remove oldest conversations
        if len(self.conversaciones_activas) > self.MAX_CONVERSATIONS:
            # Sort by last activity time and remove oldest
            sorted_conversations = sorted(
                self.conversaciones_activas.items(),
                key=lambda x: x[1].timestamp_ultima_actividad
            )
            # Remove oldest conversations
            num_to_remove = len(self.conversaciones_activas) - self.MAX_CONVERSATIONS
            for clave, _ in sorted_conversations[:num_to_remove]:
                del self.conversaciones_activas[clave]
        
        if keys_to_remove or len(self.conversaciones_activas) > self.MAX_CONVERSATIONS:
            # Use print if logger not available
            try:
                import logging
                logger = logging.getLogger(__name__)
                logger.info(f"Cleaned up {len(keys_to_remove)} old conversations. Active: {len(self.conversaciones_activas)}")
            except:
                print(f"Cleaned up {len(keys_to_remove)} old conversations. Active: {len(self.conversaciones_activas)}")

    def _actualizar_contexto(self, contexto: ContextoConversacion, mensaje: str):
        """Actualiza el contexto con un nuevo mensaje"""
        contexto.mensajes_intercambiados.append(
            {
                "tipo": "cliente",
                "mensaje": mensaje,
                "timestamp": datetime.datetime.now().isoformat(),
            }
        )
        # Limit message history to prevent unbounded memory growth
        if len(contexto.mensajes_intercambiados) > self.MAX_MESSAGES_PER_CONVERSATION:
            # Keep only the most recent messages
            contexto.mensajes_intercambiados = contexto.mensajes_intercambiados[-self.MAX_MESSAGES_PER_CONVERSATION:]
        contexto.timestamp_ultima_actividad = datetime.datetime.now()

    def _agentic_processing(self, mensaje: str, contexto: ContextoConversacion) -> RespuestaIA:
        """Procesa el mensaje usando un bucle ag√©ntico con herramientas (ReAct/Tool Use)"""
        
        # 1. Preparar System Prompt y Contexto
        if not self.use_ai or not self.model_integrator:
             # Fallback si no hay IA configurada
             return self._crear_respuesta("Lo siento, estoy en modo mantenimiento. (Sin IA)", "error", 0.0, ["fallback"])

        contexto_enriquecido = self._enriquecer_contexto_completo(mensaje)
        
        # Use Standardized Quotation Agent if available
        if AGENTS_AVAILABLE:
            # Create AgentContext
            agent_ctx = AgentContext(
                session_id=contexto.sesion_id,
                history=[{"role": m.get("tipo"), "content": m.get("mensaje")} for m in contexto.mensajes_intercambiados],
                metadata={"client_data": contexto.datos_cliente}
            )
            
            # Get Agent Configuration
            agent_config = quotation_agent.run(mensaje, agent_ctx)
            
            # Use configurations from the Agent
            system_prompt = agent_config.get("system_prompt")
            tools_def = agent_config.get("tools")
            
            # Append context to system prompt if needed (or keep using the dynamic one from earlier)
            # For now, we mix the "Expert Persona" from Agent with the "Dynamic Knowledge" from here.
            system_prompt += f"\n\nCONTEXTO ADICIONAL:\n{self._construir_system_prompt(contexto_enriquecido)}"
            
        else:
            # Fallback to legacy manual construction
            system_prompt = self._construir_system_prompt(contexto_enriquecido)
            tools_def = self.crm_tools.get_tools_definition()
        
        # 2. Construir Historial para el Prompt
        
        # 2. Construir Historial para el Prompt
        # Concatenamos la historia para darle contexto al modelo (ya que generate() toma un string)
        historial_msgs = contexto.mensajes_intercambiados[-8:] # Mantener contexto relevante pero corto
        historial_str = "HISTORIAL RECIENTE:\n"
        for m in historial_msgs:
             role = "CLIENTE" if m['tipo'] == "cliente" else "ASISTENTE"
             historial_str += f"{role}: {m['mensaje']}\n"
        
        full_prompt = f"""{historial_str}
CLIENTE: {mensaje}

(Analiza la conversaci√≥n. Si tienes datos para guardar o cotizar, USA TUS HERRAMIENTAS. Si no, responde al cliente para obtenerlos.)"""

        try:
            # 3. Primera Llamada al LLM (Decisi√≥n)
            # tools_def already set above
            
            response = self.model_integrator.generate(
                prompt=full_prompt,
                system_prompt=system_prompt,
                tools=tools_def,
                tool_choice="auto",
                temperature=0.7 
            )
            
            final_content = response.get("content")
            tool_calls = response.get("tool_calls")
            
            # 4. Manejo de Herramientas
            if tool_calls:
                # El modelo quiere usar herramientas
                # Procesamos la primera herramienta (simplificaci√≥n para este paso)
                tool_call = tool_calls[0] 
                func_name = tool_call.function.name
                args_str = tool_call.function.arguments
                
                print(f"ü§ñ AGENT DECISION: Calling tool {func_name} with {args_str}")
                
                try:
                    args = json.loads(args_str)
                    tool_result = self.crm_tools.execute_tool(func_name, args)
                    
                    # 5. Segunda Llamada (Generar Respuesta con Resultado)
                    follow_up_prompt = f"""{full_prompt}

ASISTENTE (Pensamiento): He decidido usar la herramienta '{func_name}' con los par√°metros {args_str}.
RESULTADO DE LA HERRAMIENTA:
{tool_result}

INSTRUCCI√ìN:
Genera ahora la respuesta final al cliente bas√°ndote en este resultado. S√© natural, no menciones JSON ni estructuras internas."""

                    final_response = self.model_integrator.generate(
                        prompt=follow_up_prompt,
                        system_prompt=system_prompt,
                        temperature=0.7
                    )
                    final_content = final_response.get("content", "")
                    
                except json.JSONDecodeError:
                    print(f"Error decoding tool args: {args_str}")
                    final_content = "Disculpa, hubo un error t√©cnico procesando tu solicitud."
                except Exception as e:
                    print(f"Error executing tool: {e}")
                    final_content = "Disculpa, no pude completar esa acci√≥n."

            if not final_content:
                final_content = "Lo siento, no pude generar una respuesta."

            return self._crear_respuesta(
                final_content,
                "agentic_response",
                0.95,
                ["expert_agent_v1"]
            )

        except Exception as e:
            print(f"Error critical in agentic loop: {e}")
            return self._crear_respuesta(
                "Tuve un problema de conexi√≥n neuronal. ¬øMe repites eso?",
                "error",
                0.0,
                ["error_handler"]
            )


    def _analizar_intencion(self, mensaje: str) -> tuple[str, float]:
        """Analiza la intenci√≥n del mensaje del cliente con confidence scoring

        Returns:
            Tuple[str, float]: (intent, confidence_score)
        """

        mensaje_lower = mensaje.lower()

        # Patrones de intenci√≥n
        patrones_intencion = {
            "saludo": ["hola", "buenos", "buenas", "hi", "hello"],
            "despedida": ["gracias", "chau", "adios", "bye", "hasta luego"],
            "cotizacion": ["cotizar", "precio", "costo", "cuanto", "presupuesto"],
            "informacion": [
                "informacion",
                "informaci√≥n",
                "caracteristicas",
                "especificaciones",
                "que es",
                "necesito",
                "sobre",
                "acerca",
                "techos",
                "techo",
                "aislamiento",
            ],
            "producto": ["isodec", "poliestireno", "lana", "producto", "productos"],
            "instalacion": ["instalar", "instalacion", "montaje", "colocacion"],
            "servicio": ["servicio", "garantia", "soporte", "atenci√≥n"],
            "objecion": ["caro", "costoso", "no estoy seguro", "dudar"],
        }

        # Calcular puntuaci√≥n para cada intenci√≥n
        puntuaciones = {}
        for intencion, palabras in patrones_intencion.items():
            puntuacion = sum(1 for palabra in palabras if palabra in mensaje_lower)
            puntuaciones[intencion] = puntuacion

        # Retornar intenci√≥n con mayor puntuaci√≥n
        if puntuaciones:
            intencion_principal = max(puntuaciones, key=puntuaciones.get)
            if puntuaciones[intencion_principal] > 0:
                return intencion_principal

        return "general"


    def _extraer_entidades(self, mensaje: str) -> dict[str, Any]:
        """Extrae entidades del mensaje con matching mejorado"""

        mensaje_lower = mensaje.lower()
        entidades = {}

        # Extraer productos
        productos_encontrados = []
        for producto in self.entidades_reconocidas["productos"]:
            if producto in mensaje_lower:
                productos_encontrados.append(producto)
        if productos_encontrados:

            entidades["productos"] = list(set(productos_encontrados))  # Remove duplicates


        # Extraer espesores
        espesores_encontrados = []
        for espesor in self.entidades_reconocidas["espesores"]:
            if espesor in mensaje_lower:
                espesores_encontrados.append(espesor)
        if espesores_encontrados:

            entidades["espesores"] = list(set(espesores_encontrados))  # Remove duplicates


        # Extraer colores
        colores_encontrados = []
        for color in self.entidades_reconocidas["colores"]:
            if color in mensaje_lower:
                colores_encontrados.append(color)
        if colores_encontrados:
            entidades["colores"] = colores_encontrados

        # Extraer dimensiones
        dimensiones = self._extraer_dimensiones(mensaje)
        if dimensiones:
            entidades["dimensiones"] = dimensiones

        # Extraer n√∫meros de tel√©fono
        telefono = self._extraer_telefono(mensaje)
        if telefono:
            entidades["telefono"] = telefono

        # Extraer nombre y apellido
        nombre_apellido = self._extraer_nombre_apellido(mensaje)
        if nombre_apellido:
            entidades["nombre"] = nombre_apellido["nombre"]
            entidades["apellido"] = nombre_apellido["apellido"]

        return entidades

    def _extraer_dimensiones(self, mensaje: str) -> dict[str, float] | None:
        """Extrae dimensiones del mensaje"""
        # Patrones para dimensiones
        patrones = [
            r"(\d+(?:\.\d+)?)\s*[x√ó]\s*(\d+(?:\.\d+)?)",
            r"(\d+(?:\.\d+)?)\s*metros?\s*[x√ó]\s*(\d+(?:\.\d+)?)\s*metros?",
            r"(\d+(?:\.\d+)?)\s*m\s*[x√ó]\s*(\d+(?:\.\d+)?)\s*m",
        ]

        for patron in patrones:
            match = re.search(patron, mensaje, re.IGNORECASE)
            if match:
                try:
                    largo = float(match.group(1))
                    ancho = float(match.group(2))
                    return {"largo": largo, "ancho": ancho}
                except ValueError:
                    continue

        return None

    def _extraer_telefono(self, mensaje: str) -> str | None:
        """Extrae n√∫mero de tel√©fono del mensaje"""
        patron = r"(\+?598\s?)?(\d{2,3}\s?\d{3}\s?\d{3})"
        match = re.search(patron, mensaje)
        if match:
            return match.group(0).replace(" ", "")
        return None

    def _extraer_nombre_apellido(self, mensaje: str) -> dict[str, str] | None:
        """Extrae nombre y apellido del mensaje"""
        # Buscar patrones comunes de presentaci√≥n
        # Ejemplos: "Me llamo Juan Perez", "Soy Maria Rodriguez", "Juan Perez"

        # Patr√≥n: "me llamo/soy + nombre apellido"
        patron_presentacion = r"(?:me llamo|soy|mi nombre es)\s+([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)+)"
        match = re.search(patron_presentacion, mensaje, re.IGNORECASE)

        if match:
            nombre_completo = match.group(1).strip()
            partes = nombre_completo.split()
            if len(partes) >= 2:
                return {"nombre": partes[0], "apellido": " ".join(partes[1:])}

        # Patr√≥n: dos palabras capitalizadas consecutivas (sin palabras clave antes)
        # Solo si no hay otras palabras clave en el mensaje
        mensaje_limpio = mensaje.strip()
        palabras_clave = [
            "producto",
            "isodec",
            "poliestireno",
            "lana",
            "metro",
            "espesor",
            "precio",
        ]

        if not any(palabra in mensaje.lower() for palabra in palabras_clave):
            patron_nombre_simple = r"^([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)\s+([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)*)$"
            match = re.match(patron_nombre_simple, mensaje_limpio)

            if match:
                return {"nombre": match.group(1), "apellido": match.group(2)}

        return None

    def _generar_respuesta_inteligente(
        self,
        mensaje: str,
        intencion: str,
        entidades: dict[str, Any],
        contexto: ContextoConversacion,
    ) -> RespuestaIA:
        """Genera respuesta inteligente basada en el an√°lisis"""
        # Primero, intentar generar respuesta basada en intenci√≥n detectada
        # Solo usar base de conocimiento si no hay intenci√≥n clara o si la intenci√≥n es "general"

        # Respuestas gen√©ricas que debemos ignorar de la base de conocimiento
        respuestas_genericas = [
            "gracias por tu consulta",
            "te ayudo con la informaci√≥n",
            "puedo ayudarte con",
        ]

        # Si hay una intenci√≥n espec√≠fica detectada, usarla primero
        if intencion != "general":
            if intencion == "saludo":
                return self._manejar_saludo(contexto)
            elif intencion == "despedida":
                return self._manejar_despedida(contexto)
            elif intencion == "cotizacion":
                return self._manejar_cotizacion(entidades, contexto)
            elif intencion == "informacion":
                return self._manejar_informacion(entidades, contexto)
            elif intencion == "producto":
                return self._manejar_consulta_producto(entidades, contexto)
            elif intencion == "objecion":
                return self._manejar_objecion(mensaje, contexto)

        # Si intenci√≥n es "general", buscar en base de conocimiento
        # pero solo si la respuesta no es gen√©rica
        respuesta_conocimiento = self.base_conocimiento.obtener_respuesta_inteligente(
            mensaje, contexto.datos_cliente
        )

        if respuesta_conocimiento and len(respuesta_conocimiento) > 50:
            # Verificar si la respuesta es gen√©rica
            respuesta_lower = respuesta_conocimiento.lower()
            es_generica = any(generica in respuesta_lower for generica in respuestas_genericas)

            if not es_generica:
                # Usar respuesta de la base de conocimiento si no es gen√©rica
                return self._crear_respuesta(
                    respuesta_conocimiento, "informativa", 0.8, ["base_conocimiento"]
                )

        # Si llegamos aqu√≠, usar respuesta general
        return self._manejar_consulta_general(mensaje, contexto)

    def _manejar_saludo(self, contexto: ContextoConversacion) -> RespuestaIA:
        """Maneja saludos del cliente usando IA"""
        return self._generar_saludo_ia(contexto)
    
    def _generar_saludo_ia(self, contexto: ContextoConversacion) -> RespuestaIA:
        """Genera saludo usando IA con contexto de base de conocimiento"""
        if not self.use_ai or not self.model_integrator:
            # Fallback m√≠nimo si no hay IA
            mensaje = "¬°Hola! Soy tu asistente de cotizaciones de BMC Uruguay. ¬øEn qu√© puedo ayudarte?"
            return self._crear_respuesta(mensaje, "informativa", 0.7, ["fallback"])
        
        # Enriquecer contexto
        contexto_enriquecido = self._enriquecer_contexto_completo("Hola, quiero informaci√≥n sobre productos", "saludo")
        system_prompt = self._construir_system_prompt(contexto_enriquecido)
        
        user_prompt = "El cliente dice 'hola' o saluda. Genera un saludo amigable y profesional present√°ndote como Superchapita, asistente de BMC Uruguay."
        
        try:
            response = self.model_integrator.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.8,
                max_tokens=150
            )
            
            mensaje = response.strip() if isinstance(response, str) else str(response)
            return self._crear_respuesta(mensaje, "informativa", 0.9, ["ia", "base_conocimiento"])
        except Exception as e:
            print(f"[ERROR] Error generando saludo con IA: {e}")
            mensaje = "¬°Hola! Soy tu asistente de cotizaciones de BMC Uruguay. ¬øEn qu√© puedo ayudarte?"
            return self._crear_respuesta(mensaje, "informativa", 0.7, ["fallback"])

    def _manejar_despedida(self, contexto: ContextoConversacion) -> RespuestaIA:
        """Maneja despedidas del cliente usando IA"""
        return self._generar_despedida_ia(contexto)
    
    def _generar_despedida_ia(self, contexto: ContextoConversacion) -> RespuestaIA:
        """Genera despedida usando IA con contexto de la conversaci√≥n"""
        if not self.use_ai or not self.model_integrator:
            mensaje = "¬°Gracias por contactar BMC Uruguay! Que tengas un excelente d√≠a."
            return self._crear_respuesta(mensaje, "despedida", 0.7, ["fallback"])
        
        # Construir contexto de la conversaci√≥n
        historial_resumen = ""
        if contexto.mensajes_intercambiados:
            ultimos = contexto.mensajes_intercambiados[-3:]
            historial_resumen = "\n".join([f"{m.get('tipo', '')}: {m.get('mensaje', '')[:100]}" for m in ultimos])
        
        system_prompt = "Eres Superchapita, asistente de BMC Uruguay. Genera despedidas amigables y profesionales."
        user_prompt = f"El cliente se despide. Contexto de la conversaci√≥n:\n{historial_resumen}\n\nGenera una despedida apropiada."
        
        try:
            response = self.model_integrator.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=100
            )
            
            mensaje = response.strip() if isinstance(response, str) else str(response)
            return self._crear_respuesta(mensaje, "despedida", 0.9, ["ia"])
        except Exception as e:
            print(f"[ERROR] Error generando despedida con IA: {e}")
            mensaje = "¬°Gracias por contactar BMC Uruguay! Que tengas un excelente d√≠a."
            return self._crear_respuesta(mensaje, "despedida", 0.7, ["fallback"])

    def _manejar_cotizacion(
        self, entidades: dict[str, Any], contexto: ContextoConversacion
    ) -> RespuestaIA:
        """Maneja solicitudes de cotizaci√≥n"""
        if contexto.estado_cotizacion == "inicial":
            contexto.estado_cotizacion = "recopilando_datos"
            mensaje = (
                "¬°Perfecto! Vamos a crear tu cotizaci√≥n paso a paso.\n\n"
                "Necesito algunos datos:\n"
                "1Ô∏è‚É£ ¬øCu√°l es tu nombre y apellido?\n"
                "2Ô∏è‚É£ ¬øCu√°l es tu tel√©fono?\n"
                "3Ô∏è‚É£ ¬øQu√© producto te interesa? (Isodec, Poliestireno, Lana de Roca)\n"
                "4Ô∏è‚É£ ¬øCu√°les son las dimensiones? (largo x ancho en metros)\n"
                "5Ô∏è‚É£ ¬øQu√© espesor necesitas? (50mm, 75mm, 100mm, 125mm, 150mm)"
            )

            return self._crear_respuesta(mensaje, "pregunta", 0.9, ["sistema_cotizaciones"])

        elif contexto.estado_cotizacion == "recopilando_datos":
            # Actualizar datos del cliente con entidades extra√≠das
            if "nombre" in entidades:
                contexto.datos_cliente["nombre"] = entidades["nombre"]

            if "apellido" in entidades:
                contexto.datos_cliente["apellido"] = entidades["apellido"]

            if "telefono" in entidades:
                contexto.datos_cliente["telefono"] = entidades["telefono"]

            # Actualizar datos del producto con entidades extra√≠das
            if "productos" in entidades:
                contexto.datos_producto["producto"] = entidades["productos"][0]

            if "dimensiones" in entidades:
                contexto.datos_producto["largo"] = entidades["dimensiones"]["largo"]
                contexto.datos_producto["ancho"] = entidades["dimensiones"]["ancho"]

            if "espesores" in entidades:
                contexto.datos_producto["espesor"] = entidades["espesores"][0]

            # Construir contexto de validaci√≥n unificado
            contexto_validacion = construir_contexto_validacion(
                contexto.datos_cliente, contexto.datos_producto
            )

            # Usar validaci√≥n centralizada para verificar datos faltantes
            datos_faltantes = obtener_datos_faltantes(contexto_validacion)

            if datos_faltantes:
                # Hay datos faltantes, solicitar al cliente
                mensaje = formatear_mensaje_faltantes(datos_faltantes)
                return self._crear_respuesta(mensaje, "pregunta", 0.8, ["sistema_cotizaciones"])

            # Todos los datos est√°n completos, crear cotizaci√≥n
            cotizacion = self._crear_cotizacion(contexto)
            if cotizacion:
                contexto.estado_cotizacion = "cotizacion_completada"
                mensaje = self._formatear_cotizacion(cotizacion)
                return self._crear_respuesta(mensaje, "cotizacion", 0.95, ["sistema_cotizaciones"])
            else:
                # Error al crear cotizaci√≥n
                return self._crear_respuesta(
                    "Hubo un error al generar la cotizaci√≥n. ¬øPodr√≠as verificar los datos?",
                    "pregunta",
                    0.5,
                    ["sistema_cotizaciones"],
                )

        return self._crear_respuesta("¬øEn qu√© m√°s puedo ayudarte?", "pregunta", 0.7, ["general"])

    def _manejar_informacion(
        self, entidades: dict[str, Any], contexto: ContextoConversacion
    ) -> RespuestaIA:
        """Maneja solicitudes de informaci√≥n"""
        if "productos" in entidades:
            producto = entidades["productos"][0]
            mensaje = self._obtener_informacion_producto(producto)
        else:
            mensaje = (
                "Tenemos varios productos de aislamiento t√©rmico:\n\n"
                "üè† **ISODEC** - Panel aislante con n√∫cleo EPS\n"
                "üß± **POLIESTIRENO** - Aislante b√°sico\n"
                "ü™® **LANA DE ROCA** - Aislante t√©rmico y ac√∫stico\n\n"
                "¬øSobre cu√°l te gustar√≠a saber m√°s?"
            )

        return self._crear_respuesta(mensaje, "informativa", 0.9, ["base_conocimiento"])

    def _manejar_consulta_producto(
        self, entidades: dict[str, Any], contexto: ContextoConversacion
    ) -> RespuestaIA:
        """Maneja consultas espec√≠ficas sobre productos"""
        if "productos" in entidades:
            producto = entidades["productos"][0]
            mensaje = self._obtener_informacion_producto(producto)
        else:
            mensaje = "¬øSobre qu√© producto espec√≠fico te gustar√≠a informaci√≥n?"

        return self._crear_respuesta(mensaje, "informativa", 0.8, ["base_conocimiento"])

    def _manejar_objecion(self, mensaje: str, contexto: ContextoConversacion) -> RespuestaIA:
        """Maneja objeciones del cliente"""
        mensaje_lower = mensaje.lower()

        if "caro" in mensaje_lower or "costoso" in mensaje_lower:
            respuesta = (
                "Entiendo tu preocupaci√≥n por el precio. Te explico el valor a largo plazo:\n\n"
                "‚úÖ Ahorro energ√©tico del 30-40%\n"
                "‚úÖ Durabilidad superior a 20 a√±os\n"
                "‚úÖ Incluye instalaci√≥n y garant√≠a\n"
                "‚úÖ Retorno de inversi√≥n en 3-5 a√±os\n\n"
                "¬øTe gustar√≠a que te muestre un c√°lculo de ahorro espec√≠fico?"
            )
        elif "no estoy seguro" in mensaje_lower:
            respuesta = (
                "Es normal tener dudas en una inversi√≥n importante. Te puedo ayudar:\n\n"
                "üìã Enviarte informaci√≥n detallada\n"
                "üìû Conectarte con nuestro t√©cnico\n"
                "üè† Mostrarte casos similares exitosos\n\n"
                "¬øQu√© te ayudar√≠a a decidir?"
            )
        else:
            respuesta = (
                "Entiendo tu preocupaci√≥n. ¬øPodr√≠as contarme m√°s espec√≠ficamente qu√© te preocupa?"
            )

        return self._crear_respuesta(
            respuesta, "informativa", 0.8, ["base_conocimiento", "objeciones"]
        )

    def _manejar_consulta_general(
        self, mensaje: str, contexto: ContextoConversacion
    ) -> RespuestaIA:
        """Maneja consultas generales usando IA"""
        if not self.use_ai or not self.model_integrator:
            mensaje_respuesta = (
            "Puedo ayudarte con:\n\n"
            "üè† Informaci√≥n sobre productos de aislamiento\n"
            "üí∞ Cotizaciones personalizadas\n"
            "üìã Especificaciones t√©cnicas\n"
            "üîß Consultas sobre instalaci√≥n\n\n"
            "¬øEn qu√© te gustar√≠a que te ayude?"
        )
            return self._crear_respuesta(mensaje_respuesta, "informativa", 0.7, ["fallback"])
        
        # Enriquecer contexto
        contexto_enriquecido = self._enriquecer_contexto_completo(mensaje, "consulta_general")
        system_prompt = self._construir_system_prompt(contexto_enriquecido)
        
        user_prompt = f"El cliente pregunta: {mensaje}\n\nGenera una respuesta √∫til y contextual basada en la informaci√≥n disponible."
        
        try:
            response = self.model_integrator.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=300
            )
            
            mensaje_respuesta = response.strip() if isinstance(response, str) else str(response)
            return self._crear_respuesta(mensaje_respuesta, "informativa", 0.8, ["ia", "base_conocimiento"])
        except Exception as e:
            print(f"[ERROR] Error manejando consulta general con IA: {e}")
            mensaje_respuesta = "Puedo ayudarte con informaci√≥n sobre productos, cotizaciones y especificaciones t√©cnicas. ¬øEn qu√© te gustar√≠a que te ayude?"
            return self._crear_respuesta(mensaje_respuesta, "informativa", 0.7, ["fallback"])

    def _obtener_informacion_producto(self, producto: str) -> str:
        """Obtiene informaci√≥n detallada de un producto usando IA"""
        return self._obtener_informacion_producto_ia(producto)
    
    def _obtener_informacion_producto_ia(self, producto: str) -> str:
        """Obtiene informaci√≥n de producto usando IA y base de conocimiento"""
        # Obtener informaci√≥n del producto desde Knowledge Manager
        info_producto = None
        if self.knowledge_manager:
            info_producto = self.knowledge_manager.obtener_info_producto(producto)
        
        # Si no hay IA disponible, usar informaci√≥n b√°sica
        if not self.use_ai or not self.model_integrator:
            if info_producto:
                return json.dumps(info_producto, indent=2, ensure_ascii=False)
            return f"Informaci√≥n sobre {producto} no disponible en este momento."
        
        # Construir prompt con informaci√≥n del producto
        info_texto = ""
        if info_producto:
            info_texto = json.dumps(info_producto, indent=2, ensure_ascii=False)
        else:
            info_texto = f"Producto: {producto}\nNo se encontr√≥ informaci√≥n detallada en la base de conocimiento."
        
        system_prompt = """Eres Superchapita, asistente experto en productos de aislamiento t√©rmico de BMC Uruguay.
Genera respuestas informativas, naturales y conversacionales sobre productos.
Incluye caracter√≠sticas, precios, especificaciones y opciones disponibles."""
        
        user_prompt = f"""Un cliente pregunta sobre el producto: {producto}

Informaci√≥n disponible del producto:
{info_texto}

Genera una respuesta natural y completa sobre este producto, incluyendo:
- Caracter√≠sticas principales
- Precios (si est√°n disponibles)
- Opciones disponibles (espesores, colores, terminaciones)
- Ventajas y beneficios

Responde de forma conversacional y amigable."""
        
        try:
            response = self.model_integrator.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=400
            )
            
            return response.strip() if isinstance(response, str) else str(response)
        except Exception as e:
            print(f"[ERROR] Error obteniendo informaci√≥n de producto con IA: {e}")
            if info_producto:
                return json.dumps(info_producto, indent=2, ensure_ascii=False)
            return f"Informaci√≥n sobre {producto} no disponible en este momento."

    def _crear_cotizacion(self, contexto: ContextoConversacion):
        """Crea una cotizaci√≥n basada en los datos del contexto"""
        try:
            # Combinar nombre y apellido para el campo nombre del cliente
            nombre_completo = contexto.datos_cliente.get("nombre", "Cliente")
            apellido = contexto.datos_cliente.get("apellido", "")
            if apellido:
                nombre_completo = f"{nombre_completo} {apellido}"

            # Crear cliente
            cliente = Cliente(
                nombre=nombre_completo,
                telefono=contexto.datos_cliente.get("telefono", ""),
                direccion=contexto.datos_cliente.get("direccion", ""),
                zona=contexto.datos_cliente.get("zona", ""),
            )

            # Crear especificaciones
            especificaciones = EspecificacionCotizacion(
                producto=contexto.datos_producto.get("producto", "isodec"),
                espesor=contexto.datos_producto.get("espesor", "100mm"),
                relleno="EPS",
                largo_metros=Decimal(str(contexto.datos_producto.get("largo", 5))),
                ancho_metros=Decimal(str(contexto.datos_producto.get("ancho", 3))),
                color=contexto.datos_producto.get("color", "Blanco"),
                termina_front="Gotero",
                termina_sup="Gotero",
                termina_lat_1="Gotero",
                termina_lat_2="Gotero",
                anclajes="Incluido",
                traslado="Incluido",
            )

            # Crear cotizaci√≥n
            cotizacion = self.sistema_cotizaciones.crear_cotizacion(
                cliente=cliente,
                especificaciones=especificaciones,
                asignado_a="IA",
                observaciones="Cotizaci√≥n generada por IA conversacional",
            )

            return cotizacion

        except Exception as e:
            print(f"Error creando cotizaci√≥n: {e}")
            return None

    def _formatear_cotizacion(self, cotizacion) -> str:
        """Formatea una cotizaci√≥n para mostrar al cliente"""
        area = cotizacion.especificaciones.largo_metros * cotizacion.especificaciones.ancho_metros

        return (
            f"üéâ **¬°COTIZACI√ìN LISTA!**\n\n"
            f"üìã **ID:** {cotizacion.id}\n"
            f"üè† **Producto:** {cotizacion.especificaciones.producto.upper()}\n"
            f"üìè **Dimensiones:** {cotizacion.especificaciones.largo_metros}m x {cotizacion.especificaciones.ancho_metros}m\n"
            f"üìê **√Årea total:** {area} m¬≤\n"
            f"üìê **Espesor:** {cotizacion.especificaciones.espesor}\n"
            f"üé® **Color:** {cotizacion.especificaciones.color}\n\n"
            f"üí∞ **PRECIO POR M¬≤:** ${cotizacion.precio_metro_cuadrado}\n"
            f"üí∞ **PRECIO TOTAL:** ${cotizacion.precio_total}\n\n"
            f"‚úÖ **Incluye:** Material, terminaciones, anclajes y traslado\n\n"
            f"¬øTe parece bien esta cotizaci√≥n? ¬øNecesitas alg√∫n ajuste?"
        )

    def _crear_respuesta(
        self, mensaje: str, tipo: str, confianza: float, fuentes: list[str]
    ) -> RespuestaIA:
        """Crea una respuesta estructurada"""
        return RespuestaIA(
            mensaje=mensaje,
            tipo_respuesta=tipo,
            acciones_sugeridas=[],
            confianza=confianza,
            fuentes_conocimiento=fuentes,
            personalizacion={},
            timestamp=datetime.datetime.now(),
        )

    def _registrar_interaccion(
        self,
        mensaje_cliente: str,
        respuesta: RespuestaIA,
        contexto: ContextoConversacion,
    ):
        """Registra la interacci√≥n en la base de conocimiento"""
        interaccion = InteraccionCliente(
            id=f"ia_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}",
            timestamp=datetime.datetime.now(),
            cliente_id=contexto.cliente_id,
            tipo_interaccion="consulta_ia",
            mensaje_cliente=mensaje_cliente,
            respuesta_agente=respuesta.mensaje,
            contexto=contexto.datos_cliente,
            resultado="exitoso" if respuesta.confianza > 0.7 else "pendiente",
            satisfaccion_cliente=None,
        )

        self.base_conocimiento.registrar_interaccion(interaccion)

    def _actualizar_conocimiento_conversacion(
        self, contexto: ContextoConversacion, respuesta: RespuestaIA
    ):
        """Actualiza el conocimiento basado en la conversaci√≥n"""
        # Actualizar patrones de respuesta si la respuesta fue efectiva
        if respuesta.confianza > 0.8:
            tipo_respuesta = respuesta.tipo_respuesta
            if tipo_respuesta not in self.patrones_respuesta:
                self.patrones_respuesta[tipo_respuesta] = []

            if respuesta.mensaje not in self.patrones_respuesta[tipo_respuesta]:
                self.patrones_respuesta[tipo_respuesta].append(respuesta.mensaje)

    def procesar_mensaje_usuario(

        self, mensaje: str, telefono_cliente: str, sesion_id: str = None
    ) -> dict[str, Any]:

        """
        Procesa mensaje del usuario usando IA para respuestas naturales y contextuales.
        Siempre usa OpenAI cuando est√° disponible para generar respuestas fluidas e inteligentes.
        Retorna diccionario compatible con API
        
        Args:
            mensaje: Mensaje del usuario
            telefono_cliente: Tel√©fono del cliente
            sesion_id: ID de sesi√≥n (opcional)
            request_id: Request ID para tracking (opcional)
            client_request_id: Client request ID para tracking (opcional)
        """
        if not sesion_id:
            sesion_id = f"sesion_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}"
        request_id = None
        client_request_id = None


        # Usar IA obligatoria - NO fallback a pattern matching
        if self.use_ai and self.model_integrator:
            try:
                return self._procesar_con_ia(
                    mensaje, telefono_cliente, sesion_id,
                    request_id=request_id, client_request_id=client_request_id
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Error con model_integrator: {e}")
                # Intentar con otro modelo si est√° disponible
                if self.openai_client:
                    try:
                        return self._procesar_con_openai_fallback(
                            mensaje, telefono_cliente, sesion_id
                        )
                    except Exception as e2:
                        print(f"‚ö†Ô∏è Error con OpenAI fallback: {e2}")
                        raise Exception("Todos los modelos de IA fallaron. No se puede procesar el mensaje.")
                else:
                    raise Exception("Model integrator fall√≥ y no hay fallback disponible.")
        elif self.use_ai and self.openai_client:
            # Fallback a OpenAI directo si model_integrator no est√° disponible
            try:
                return self._procesar_con_openai_fallback(
                    mensaje, telefono_cliente, sesion_id
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Error con OpenAI: {e}")
                raise Exception("IA no disponible. No se puede procesar el mensaje.")
        else:
            raise Exception("IA no disponible. Configure al menos un modelo de IA.")

    def _procesar_con_ia(
        self, mensaje: str, telefono_cliente: str, sesion_id: str,
        request_id: Optional[str] = None, client_request_id: Optional[str] = None
    ) -> dict[str, Any]:
        """Procesa mensaje usando model_integrator (m√©todo principal)"""
        
        # Obtener contexto
        contexto = self._obtener_contexto_conversacion(telefono_cliente, sesion_id)
        
        # Enriquecer contexto completo
        contexto_enriquecido = self._enriquecer_contexto_completo(mensaje)
        
        # Construir system prompt
        system_prompt = self._construir_system_prompt(contexto_enriquecido)
        
        # Obtener historial reciente
        historial = (
            contexto.mensajes_intercambiados[-5:]
            if len(contexto.mensajes_intercambiados) > 5
            else contexto.mensajes_intercambiados
        )
        
        # Construir mensaje con historial
        historial_texto = ""
        for msg in historial:
            if msg.get("tipo") == "cliente":
                historial_texto += f"Usuario: {msg.get('mensaje', '')}\n"
            else:
                historial_texto += f"Asistente: {msg.get('mensaje', '')}\n"
        
        user_prompt = f"{historial_texto}Usuario: {mensaje}\n\nAsistente:"
        
        # Generar respuesta con model_integrator
        try:
            response = self.model_integrator.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=500
            )
            
            mensaje_respuesta = response.strip() if isinstance(response, str) else str(response)
            
            # Procesar respuesta exitosa para aprendizaje
            if self.training_system:
                conversacion_data = {
                    'mensaje_cliente': mensaje,
                    'respuesta_bot': mensaje_respuesta,
                    'confianza': 0.9,
                    'completada': False
                }
                # Procesar en background (no bloquear)
                try:
                    self.training_system.procesar_conversacion_para_aprendizaje(conversacion_data)
                except Exception as e:
                    print(f"[WARNING] Error procesando conversaci√≥n para aprendizaje: {e}")
            
            # Actualizar contexto
            contexto.mensajes_intercambiados.append({
                "tipo": "cliente",
                "mensaje": mensaje,
                "timestamp": datetime.datetime.now().isoformat()
            })
            contexto.mensajes_intercambiados.append({
                "tipo": "asistente",
                "mensaje": mensaje_respuesta,
                "timestamp": datetime.datetime.now().isoformat()
            })
            contexto.timestamp_ultima_actividad = datetime.datetime.now()
            
            return {
                "mensaje": mensaje_respuesta,
                "tipo": "general",
                "acciones": [],
                "confianza": 0.9,
                "necesita_datos": [],
                "fuente": "model_integrator",
                "timestamp": datetime.datetime.now().isoformat(),
                "sesion_id": sesion_id
            }
            
        except Exception as e:
            print(f"[ERROR] Error en _procesar_con_ia: {e}")
            raise
    
    def _procesar_con_openai_fallback(
        self, mensaje: str, telefono_cliente: str, sesion_id: str
    ) -> dict[str, Any]:
        """Procesa mensaje usando OpenAI como fallback"""
        
        # Obtener contexto
        contexto = self._obtener_contexto_conversacion(telefono_cliente, sesion_id)
        
        # Obtener historial reciente
        historial = (
            contexto.mensajes_intercambiados[-5:]
            if len(contexto.mensajes_intercambiados) > 5
            else contexto.mensajes_intercambiados
        )
        
        # Obtener informaci√≥n de productos
        info_productos = self._obtener_info_productos_para_prompt()
        estado_cotizacion = self._obtener_estado_cotizacion_para_prompt(contexto)
        
        # Construir mensajes
        messages = [
            {
                "role": "system",
                "content": f"""Eres Superchapita, un asistente experto en ventas de productos de construcci√≥n de BMC Uruguay.
Tu trabajo es ayudar a los clientes con:
1. Informaci√≥n sobre productos de aislamiento t√©rmico (Isodec, Poliestireno, Lana de Roca)
2. Cotizaciones personalizadas
3. Consultas t√©cnicas
4. Seguimiento de pedidos

{info_productos}

{estado_cotizacion}

INSTRUCCIONES:
- Responde de forma FLUIDA, NATURAL y CONVERSACIONAL en espa√±ol de Uruguay
- NUNCA repitas informaci√≥n que ya compartiste
- Var√≠a tus respuestas
- S√© CONCISO
- Usa emojis moderadamente (1-2 por mensaje m√°ximo)
- Mant√©n el tono profesional pero amigable"""
            }
        ]
        
        # Agregar historial
        for msg in historial:
            if msg["tipo"] == "cliente":
                messages.append({"role": "user", "content": msg["mensaje"]})
            else:
                messages.append({"role": "assistant", "content": msg["mensaje"]})
        
        # Agregar mensaje actual
        messages.append({"role": "user", "content": mensaje})
        
        # Llamar a OpenAI
        response = self.openai_client.chat.completions.create(
            model=self.openai_model,
            messages=messages,
            temperature=0.7,
            max_tokens=500
        )
        
        mensaje_respuesta = response.choices[0].message.content.strip()
        
        # Actualizar contexto
        contexto.mensajes_intercambiados.append({
            "tipo": "cliente",
            "mensaje": mensaje,
            "timestamp": datetime.datetime.now().isoformat()
        })
        contexto.mensajes_intercambiados.append({
            "tipo": "asistente",
            "mensaje": mensaje_respuesta,
            "timestamp": datetime.datetime.now().isoformat()
        })
        contexto.timestamp_ultima_actividad = datetime.datetime.now()
        
        return {
            "mensaje": mensaje_respuesta,
            "tipo": "general",
            "acciones": [],
            "confianza": 0.85,
            "necesita_datos": [],
            "fuente": "openai_fallback",
            "timestamp": datetime.datetime.now().isoformat(),
            "sesion_id": sesion_id
        }
    
    def _procesar_con_openai_OLD(
        self, mensaje: str, telefono_cliente: str, sesion_id: str
    ) -> dict[str, Any]:
        """Procesa mensaje usando OpenAI"""

        # Obtener contexto
        contexto = self._obtener_contexto_conversacion(telefono_cliente, sesion_id)

        # Obtener historial reciente
        historial = (
            contexto.mensajes_intercambiados[-5:]
            if len(contexto.mensajes_intercambiados) > 5
            else contexto.mensajes_intercambiados
        )

        # Obtener informaci√≥n de productos y precios para enriquecer el contexto
        info_productos = self._obtener_info_productos_para_prompt()
        estado_cotizacion = self._obtener_estado_cotizacion_para_prompt(contexto)

        # Construir historial de conversaci√≥n para OpenAI
        messages = [
            {
                "role": "system",
                "content": f"""Eres Superchapita, un asistente experto en ventas de productos de construcci√≥n de BMC Uruguay.                                   
Tu trabajo es ayudar a los clientes con:
1. Informaci√≥n sobre productos de aislamiento t√©rmico (Isodec, Poliestireno, Lana de Roca)                                                                      
2. Cotizaciones personalizadas
3. Consultas t√©cnicas
4. Seguimiento de pedidos

{info_productos}

{estado_cotizacion}

INSTRUCCIONES CR√çTICAS PARA CONVERSACI√ìN NATURAL:
- Responde de forma FLUIDA, NATURAL y CONVERSACIONAL en espa√±ol de Uruguay
- NUNCA repitas informaci√≥n que ya compartiste en mensajes anteriores
- Si ya saludaste al cliente, NO vuelvas a presentarte ni a explicar tus capacidades
- Var√≠a tus respuestas - no uses siempre las mismas frases
- S√© CONCISO - responde directamente a lo que el cliente pregunta
- Si el cliente dice "hola" por segunda vez, responde brevemente como en una conversaci√≥n real
- Si el cliente solicita una cotizaci√≥n, pide los datos necesarios: producto, dimensiones (largo x ancho), espesor, color
- Usa emojis moderadamente (1-2 por mensaje m√°ximo)
- Mant√©n el tono profesional pero amigable
- Adapta tu respuesta al contexto de la conversaci√≥n - lee el historial antes de responder

IMPORTANTE: Debes responder SIEMPRE en formato JSON con esta estructura exacta:
{{
  "mensaje": "tu respuesta al cliente aqu√≠",
  "tipo": "cotizacion|informacion|pregunta|seguimiento|general",
  "acciones": ["accion1", "accion2"],
  "confianza": 0.95,
  "necesita_datos": ["dato1", "dato2"]
}}

El campo "tipo" debe ser uno de: cotizacion, informacion, pregunta, seguimiento, general.                                                                       
El campo "confianza" debe ser un n√∫mero entre 0.0 y 1.0.
El campo "necesita_datos" debe ser una lista de datos que faltan para completar una cotizaci√≥n (ej: ["producto", "dimensiones", "espesor"]).""",
            }
        ]

        # Agregar historial
        for msg in historial:
            if msg["tipo"] == "cliente":
                messages.append({"role": "user", "content": msg["mensaje"]})
            else:
                messages.append({"role": "assistant", "content": msg["mensaje"]})

        # Agregar mensaje actual
        messages.append({"role": "user", "content": mensaje})


        # Llamar a OpenAI con temperatura m√°s alta para respuestas m√°s variadas y naturales
        response = self.openai_client.chat.completions.create(
            model=self.openai_model,
            messages=messages,
            temperature=0.85,  # Aumentado de 0.7 a 0.85 para respuestas m√°s variadas y naturales
            response_format={"type": "json_object"},
        )

        # Parsear respuesta
        resultado = json.loads(response.choices[0].message.content)


        # Actualizar contexto
        self._actualizar_contexto(contexto, mensaje)
        contexto.mensajes_intercambiados.append(
            {
                "tipo": "ia",
                "mensaje": resultado.get("mensaje", ""),
                "timestamp": datetime.datetime.now().isoformat(),
            }
        )
        # Limit message history after adding assistant message
        if len(contexto.mensajes_intercambiados) > self.MAX_MESSAGES_PER_CONVERSATION:
            contexto.mensajes_intercambiados = contexto.mensajes_intercambiados[-self.MAX_MESSAGES_PER_CONVERSATION:]

        # Crear respuesta estructurada
        fuente = ["model_integrator"] if self.model_integrator else ["openai"]
        respuesta_ia = self._crear_respuesta(
            resultado.get("mensaje", ""),
            resultado.get("tipo", "general"),
            float(resultado.get("confianza", 0.8)),
            fuente,
        )

        # Registrar interacci√≥n
        self._registrar_interaccion(mensaje, respuesta_ia, contexto)

        # Retornar formato API
        return {
            "mensaje": resultado.get("mensaje", ""),
            "tipo": resultado.get("tipo", "general"),
            "acciones": resultado.get("acciones", []),
            "confianza": float(resultado.get("confianza", 0.8)),
            "necesita_datos": resultado.get("necesita_datos", []),
            "sesion_id": sesion_id,
            "timestamp": datetime.datetime.now().isoformat(),
        }

    def _obtener_info_productos_para_prompt(self) -> str:
        """Obtiene informaci√≥n de productos para enriquecer el prompt de OpenAI"""
        productos_info = []

        # Obtener precios actuales
        precios = {
            "isodec": self.sistema_cotizaciones.obtener_precio_producto("isodec"),
            "poliestireno": self.sistema_cotizaciones.obtener_precio_producto("poliestireno"),
            "lana_roca": self.sistema_cotizaciones.obtener_precio_producto("lana_roca"),
        }

        productos_info.append("PRODUCTOS DISPONIBLES:")
        productos_info.append("1. ISODEC - Panel aislante con n√∫cleo EPS")
        productos_info.append(f"   Precio base: ${precios.get('isodec', 150):.2f} por m¬≤")
        productos_info.append(
            "   Caracter√≠sticas: Excelente aislamiento t√©rmico, f√°cil instalaci√≥n"
        )

        productos_info.append("2. POLIESTIRENO - Aislante b√°sico")
        productos_info.append(f"   Precio base: ${precios.get('poliestireno', 120):.2f} por m¬≤")
        productos_info.append("   Caracter√≠sticas: Aislante econ√≥mico y eficiente")

        productos_info.append("3. LANA DE ROCA - Aislante t√©rmico y ac√∫stico")
        productos_info.append(f"   Precio base: ${precios.get('lana_roca', 140):.2f} por m¬≤")
        productos_info.append("   Caracter√≠sticas: Aislamiento t√©rmico y ac√∫stico superior")

        productos_info.append("\nESPESORES DISPONIBLES: 50mm, 75mm, 100mm, 125mm, 150mm")
        productos_info.append("COLORES DISPONIBLES: Blanco, Gris, Beige")

        return "\n".join(productos_info)

    def _obtener_estado_cotizacion_para_prompt(self, contexto: ContextoConversacion) -> str:
        """Obtiene el estado actual de cotizaci√≥n para enriquecer el prompt"""
        if contexto.estado_cotizacion == "inicial":
            return ""

        estado_info = [f"ESTADO ACTUAL DE LA COTIZACI√ìN: {contexto.estado_cotizacion.upper()}"]

        if contexto.datos_cliente:
            estado_info.append(
                f"Datos del cliente: {json.dumps(contexto.datos_cliente, ensure_ascii=False)}"
            )

        if contexto.datos_producto:
            estado_info.append(
                f"Datos del producto: {json.dumps(contexto.datos_producto, ensure_ascii=False)}"
            )

        if contexto.estado_cotizacion == "recopilando_datos":
            datos_faltantes = []
            if not contexto.datos_producto.get("producto"):
                datos_faltantes.append("producto")
            if not contexto.datos_producto.get("largo") or not contexto.datos_producto.get("ancho"):
                datos_faltantes.append("dimensiones")
            if not contexto.datos_producto.get("espesor"):
                datos_faltantes.append("espesor")

            if datos_faltantes:
                estado_info.append(f"DATOS FALTANTES: {', '.join(datos_faltantes)}")

        return "\n".join(estado_info) if len(estado_info) > 1 else ""

    def _procesar_mensaje_patrones_DEPRECATED(
        self, mensaje: str, telefono_cliente: str, sesion_id: str
    ) -> dict[str, Any]:
        """DEPRECATED: Procesa mensaje usando pattern matching (fallback) - NO USAR, usar IA obligatoria"""
        respuesta = self.procesar_mensaje(mensaje, telefono_cliente, sesion_id)

        # Convertir RespuestaIA a formato API
        return {
            "mensaje": respuesta.mensaje,
            "tipo": respuesta.tipo_respuesta,
            "acciones": respuesta.acciones_sugeridas,
            "confianza": respuesta.confianza,
            "necesita_datos": [],
            "sesion_id": sesion_id,
            "timestamp": respuesta.timestamp.isoformat(),
        }

    def exportar_conocimiento_ia(self, archivo: str):
        """Exporta todo el conocimiento de la IA"""
        conocimiento_ia = {
            "patrones_respuesta": self.patrones_respuesta,
            "entidades_reconocidas": self.entidades_reconocidas,
            "conversaciones_activas": {
                k: {
                    "cliente_id": v.cliente_id,
                    "sesion_id": v.sesion_id,
                    "estado_cotizacion": v.estado_cotizacion,
                    "timestamp_inicio": v.timestamp_inicio.isoformat(),
                    "timestamp_ultima_actividad": v.timestamp_ultima_actividad.isoformat(),
                }
                for k, v in self.conversaciones_activas.items()
            },
            "fecha_exportacion": datetime.datetime.now().isoformat(),
        }

        with open(archivo, "w", encoding="utf-8") as f:
            json.dump(conocimiento_ia, f, ensure_ascii=False, indent=2)


def main():
    """Funci√≥n principal para demostrar la IA conversacional"""
    print("IA Conversacional Integrada BMC Uruguay")
    print("=" * 50)

    # Crear IA
    ia = IAConversacionalIntegrada()

    # Simular conversaci√≥n
    mensajes_simulados = [
        "Hola, necesito informaci√≥n sobre Isodec",
        "Quiero cotizar para mi casa, 10 metros por 5 metros",
        "100mm, blanco",
        "Perfecto, me parece bien el precio",
    ]

    cliente_id = "cliente_demo"

    for mensaje in mensajes_simulados:
        print(f"\nüë§ Cliente: {mensaje}")
        respuesta = ia.procesar_mensaje(mensaje, cliente_id)
        print(f"ü§ñ IA: {respuesta.mensaje}")
        print(f"   Confianza: {respuesta.confianza:.2f}")
        print(f"   Fuentes: {', '.join(respuesta.fuentes_conocimiento)}")

    # Exportar conocimiento
    ia.exportar_conocimiento_ia("ia_conversacional.json")
    print("\nConocimiento de IA exportado a ia_conversacional.json")


if __name__ == "__main__":
    main()
